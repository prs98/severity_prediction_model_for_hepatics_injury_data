---
title: "Homework 3"
author: "Sabbella Prasanna"
date: "4/16/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
```{r libraries, message=FALSE, warning=FALSE}
library(tidyverse)
library(AppliedPredictiveModeling)
library(caret)
library(ROCR)
library(pROC)
library(dplyr)
```
```{r dataset, message=FALSE, warning=FALSE}
data("hepatic")
injury_df=data.frame(injury)
```

1. I will be using the hepatic injury data set from the Applied Predictive Modeling textbook. 
2. The variable that I will predict is "injury".
- Variable name: injury
- injury is the intensity or level of the hipatic injury. It is described as "None" for no injury, "Mild" for mild injury and "Severe" for severe injury.
- injury is a categorical variable at three different levels with no units.
- Histogram plot for the frequency of each of the three injury levels is shown below.

```{r Histogram, message=FALSE, warning=FALSE}
ggplot(data=injury_df, aes(x=injury))+geom_bar()
```


## 3.Preprocessing the data
- *please view code*
- For simplicity of running the models "None" and "Mild" were considered as "No" injury and "Severe" is "Yes".
```{r Preprocessing, message=FALSE, warning=FALSE}
risk <- c("None" = "No", "Mild" = "No", "Severe" = "Yes")

#collapsign the responses to only two outcomes as below:
#1.Severe :  Yes
#2.Mild : No
#3.None : No

injury2 <- factor(unname(risk[injury]))
chem_bio = cbind(bio, chem)

#Remove near zero variance predictor variables/ near-zero variance) predictors.
#Remove collinear variables

chem_bio_var <- nearZeroVar(chem_bio)
chem_bio_collinear <- findLinearCombos(cov(chem_bio))$remove
chem_bio_processed <- chem_bio[, -c(chem_bio_var, chem_bio_collinear)]


bio_var <- nearZeroVar(bio)
bio_collinear <- findLinearCombos(cov(bio))$remove
bio_processed <- bio[, -c(bio_var, bio_collinear)]

```


```{r Splitting, message=FALSE, warning=FALSE}
set.seed(111)

training <- createDataPartition(injury2, p = .8, list = FALSE)

chem_bio_train = chem_bio_processed[training, ]
chem_bio_test  = chem_bio_processed[-training, ]

bio_train = bio_processed[training, ]
bio_test  = bio_processed[-training, ]

injury_train <- injury2[training]
injury_test <- injury2[-training]

temp_bio_train = cbind(bio_train, injury_train)
temp_bio_test = cbind(bio_test,injury_test )
```
## 4.Using 10 fold cross validation

## Linear discriminant analysis
```{r Linear discriminant analysis, message=FALSE, warning=FALSE}
lda_model <- train(bio_train, injury_train,
                 method = "lda", 
                 trControl = trainControl("cv", number = 10, classProbs=TRUE,summaryFunction=twoClassSummary), 
                 metric = "ROC")


bio_prediction <- predict(lda_model, bio_test)
confusionMatrix(bio_prediction, injury_test, positive = "No")


roc=roc(response=temp_bio_test$injury_test, predictor= factor(bio_prediction, 
ordered = TRUE), plot=TRUE, main="ROC curve lda_model")
```

## Trees
```{r Trees, message=FALSE, warning=FALSE}

ctrl = trainControl(method = "cv", number=10, 
                classProbs = TRUE, summaryFunction = twoClassSummary)

rf_model = train(injury_train~., data = temp_bio_train ,
                         method = "rf",
                         ntree = 50,
                         tuneLength = 10,
                         metric = "ROC",
                         trControl = ctrl)
rf_model

rf_prediction <- predict(rf_model, temp_bio_test)
rf_probability <- predict(rf_model, temp_bio_test,
type = "prob")[, "No"]

confusionMatrix(rf_prediction, as.factor(temp_bio_test$injury_test), positive = "No")

roc=roc(response=temp_bio_test$injury_test, predictor= factor(rf_probability, 
ordered = TRUE), plot=TRUE, main="ROC curve rf_model")

```

## K nearest neighbour
```{r knn, message=FALSE, warning=FALSE}
set.seed(111)
nnetGrid <- expand.grid(
decay = c(0, 1.0, 20.0, 50, 80, 100),
size = c(1, 3, 5, 7, 10, 15),
bag = TRUE
)

ctrl = trainControl(method = "cv", number=10, 
                classProbs = TRUE, summaryFunction = twoClassSummary)

knn_model = train(injury_train ~., data = temp_bio_train,
                         method = "knn",
                         trControl = ctrl)
knn_model

knn_prediction <- predict(knn_model, temp_bio_test)
knn_probability <- predict(knn_model, temp_bio_test,
type = "prob")[, "No"]

knn_probability = ifelse(knn_probability == "1", "Yes", "No")
confusionMatrix( as.factor(knn_probability), as.factor(temp_bio_test$injury_test), positive = "No")

roc_nb=roc(response=temp_bio_test$injury_test, predictor= factor(knn_probability, 
ordered = TRUE), plot=TRUE, main="ROC curve knn model")
```

## 5. Comparing the models

-   Linear Model
Linear discriminant analysis Accuracy is 80.36 %, Sensitivity : 0.8600, Specificity : 0.3333 Despite of having good accuracy and sensitivity the specificity of the model is comparitively low.

-   Trees
Random Forests Accuracy is 89.2 %, Sensitivity : 1, Specificity : 0 
Sensitivity is True positive rate and Specificity is True negative rate. Sensitivity of 1.00 and specificity of 0.00 would happen when your model classifies every positive example is correctly predicted as positive, and every negative example is incorrectly predicted as positive. 89.2 % accuracy is still possible as the data has 89.2 percent positive and the rest negative. So downsampling needs to be applied to the positive data or upsampling to the negative data or even both.

-   Non Linear Model
KNN Accuracy is 58.9, Sensitivity : 0.64, Specificity : 0.16 Linear models performed better than non linear Knn as the accuracy is low at 58 %



## 6. Model to choose.
- Out of all the models i.e linear, trees and non linear models, the trees have outperformed the linear and non linear models, especially in terms of accuracy.
- But choosing trees is not recommended as the accuracy is biased with sensitivity 1 (Explained above).
- Its save to choose the next better model which is linear discriminant analysis model. with accuracy 80%. 